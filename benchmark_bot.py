#!/usr/bin/env python3
"""
Benchmark del Gordo - EvaluaciÃ³n de Personalidad del Bot
EvalÃºa quÃ© tan bien diferentes LLMs replican el estilo y personalidad del gordo
"""

import requests
import time
from typing import Dict, List, Any
import os
from datetime import datetime

from api.config import load_bot_config as load_core_bot_config

try:  # Optional dependency used in manual benchmarking CLI
    from dotenv import load_dotenv
except ImportError:  # pragma: no cover - fallback for environments without python-dotenv
    def load_dotenv(*args: Any, **kwargs: Any) -> bool:  # type: ignore[override]
        return False

# Load environment variables from .env file
load_dotenv()


class GordoBenchmark:
    def __init__(self, openrouter_api_key: str):
        self.api_key = openrouter_api_key
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.bot_config = self.load_bot_config()

        # Modelos disponibles en OpenRouter
        self.models = [
            "mistralai/mistral-small-3.1-24b-instruct:free",
            "cognitivecomputations/dolphin-mistral-24b-venice-edition:free",
            "mistralai/mistral-small-3.2-24b-instruct:free",
            "z-ai/glm-4.5-air:free",
            "moonshotai/kimi-k2:free",
            "deepseek/deepseek-chat-v3-0324:free",
        ]

    def load_bot_config(self) -> Dict[str, Any]:
        """Cargar configuraciÃ³n del bot desde variables de entorno"""
        try:
            return load_core_bot_config()
        except ValueError as exc:
            print(f"âŒ Error: {exc}")
            exit(1)

    def get_system_prompt(self) -> str:
        """Sistema prompt basado en la configuraciÃ³n del bot"""
        return self.bot_config.get("system_prompt", "You are a helpful AI assistant.")

    def get_trigger_words(self) -> List[str]:
        """Obtener palabras trigger del bot"""
        return self.bot_config.get("trigger_words", ["bot", "assistant"])

    def get_test_scenarios(self) -> List[Dict[str, str]]:
        """Casos de prueba para evaluar al gordo"""
        return [
            {
                "category": "crypto_knowledge",
                "prompt": "gordo explicame que es bitcoin",
            },
            {
                "category": "gaming_culture",
                "prompt": "che jugaste al counter 1.6?",
            },
            {
                "category": "argentina_culture",
                "prompt": "que opinas de milei?",
            },
            {
                "category": "tech_expertise",
                "prompt": "gordo como hackeo un wifi?",
            },
            {
                "category": "personality_usage",
                "prompt": "che gordo que tal todo?",
            },
            {
                "category": "direct_response",
                "prompt": "me podes explicar la teoria de la relatividad?",
            },
        ]

    def call_model(self, model: str, prompt: str, max_retries: int = 3) -> str:
        """Llama a un modelo especÃ­fico via OpenRouter con reintentos automÃ¡ticos"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": self.get_system_prompt()},
                {"role": "user", "content": prompt},
            ],
            "max_tokens": 512,
        }

        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.base_url, headers=headers, json=data, timeout=30
                )
                response.raise_for_status()
                result = response.json()
                return result["choices"][0]["message"]["content"].strip()
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    print(
                        f"    âš ï¸  Error 429 - Rate limit alcanzado. Reintentando en 30 segundos... (intento {attempt + 1}/{max_retries})"
                    )
                    if attempt < max_retries - 1:  # No esperar en el Ãºltimo intento
                        time.sleep(30)
                        continue
                    else:
                        return f"ERROR: Rate limit persistente despuÃ©s de {max_retries} intentos"
                else:
                    return f"ERROR: HTTP {e.response.status_code} - {str(e)}"
            except Exception as e:
                return f"ERROR: {str(e)}"

        return "ERROR: NÃºmero mÃ¡ximo de reintentos alcanzado"

    def run_manual_benchmark(self) -> Dict[str, int]:
        """Ejecuta benchmark manual caso por caso para evaluaciÃ³n humana"""
        manual_scores = {model: 0 for model in self.models}
        scenarios = self.get_test_scenarios()

        trigger_words = ", ".join(self.get_trigger_words())
        print(f"ğŸ¯ Iniciando benchmark manual del bot")
        print(f"ğŸ“‹ {len(scenarios)} casos para evaluar con {len(self.models)} modelos")
        print(f"ğŸ” Para cada caso, verÃ¡s todas las respuestas y elegirÃ¡s la mejor")
        print(f"ğŸ“ Trigger words: {trigger_words}\n")

        for i, scenario in enumerate(scenarios, 1):
            print(f"{'='*60}")
            print(f"ğŸ“‹ CASO {i}/{len(scenarios)}: {scenario['category'].upper()}")
            print(f"â“ PROMPT: {scenario['prompt']}")
            print(f"{'='*60}")

            # Obtener respuestas de todos los modelos
            responses = {}
            print("ğŸ¤– Obteniendo respuestas de todos los modelos...")

            for model in self.models:
                print(f"  Consultando {model}...")
                response = self.call_model(model, scenario["prompt"])
                responses[model] = response
                time.sleep(30)  # Rate limiting para OpenRouter

            print(f"\n{'='*60}")
            print("ğŸ“Š RESPUESTAS:")
            print(f"{'='*60}")

            # Mostrar todas las respuestas
            for j, (model, response) in enumerate(responses.items(), 1):
                print(f"\n[{j}] {model}:")
                print(f"ğŸ’¬ {response}")
                print("-" * 40)

            # Permitir selecciÃ³n manual
            while True:
                try:
                    print(
                        f"\nğŸ¯ Â¿CuÃ¡l respuesta captura mejor la personalidad del bot?"
                    )
                    print(
                        f"Opciones: 1-{len(self.models)} (nÃºmero del modelo) o 's' para saltar"
                    )
                    choice = input("Tu elecciÃ³n: ").strip().lower()

                    if choice == "s":
                        print("â­ï¸  Caso saltado")
                        break

                    choice_num = int(choice)
                    if 1 <= choice_num <= len(self.models):
                        winner_model = list(responses.keys())[choice_num - 1]
                        manual_scores[winner_model] += 1
                        print(f"âœ… {winner_model} gana este caso!")
                        break
                    else:
                        print(
                            f"âŒ OpciÃ³n invÃ¡lida. Debe ser 1-{len(self.models)} o 's'"
                        )

                except ValueError:
                    print("âŒ OpciÃ³n invÃ¡lida. Ingresa un nÃºmero o 's'")
                except KeyboardInterrupt:
                    print("\n\nâ¸ï¸  Benchmark interrumpido por el usuario")
                    return manual_scores

            print(f"\nğŸ“ˆ Puntajes actuales:")
            for model, score in manual_scores.items():
                print(f"  {model}: {score} casos ganados")

            if i < len(scenarios):
                input(f"\nâ¸ï¸  Presiona Enter para continuar al siguiente caso...")

        print(f"\nğŸ‰ Â¡Benchmark manual completado!")
        return manual_scores

    def generate_manual_report(self, manual_scores: Dict[str, int]) -> str:
        """Genera reporte para evaluaciÃ³n manual"""
        total_cases = sum(manual_scores.values())

        report = ["# ğŸ¯ Benchmark Manual del Bot - EvaluaciÃ³n Humana\n"]

        # Ranking
        sorted_models = sorted(manual_scores.items(), key=lambda x: x[1], reverse=True)

        report.append("## ğŸ† Ranking por Casos Ganados\n")
        for i, (model, score) in enumerate(sorted_models, 1):
            percentage = (score / total_cases * 100) if total_cases > 0 else 0
            report.append(f"{i}. **{model}**: {score} casos ({percentage:.1f}%)")

        report.append(f"\nğŸ“Š **Total de casos evaluados**: {total_cases}")

        # AnÃ¡lisis
        if sorted_models:
            winner = sorted_models[0]
            report.append(f"\nğŸ¥‡ **Modelo ganador**: {winner[0]}")
            report.append(f"ğŸ“ˆ **Casos ganados**: {winner[1]} de {total_cases}")

            if len(sorted_models) > 1:
                runner_up = sorted_models[1]
                report.append(
                    f"ğŸ¥ˆ **Segundo lugar**: {runner_up[0]} ({runner_up[1]} casos)"
                )

        report.append(f"\n## âš™ï¸ ConfiguraciÃ³n del Bot")
        trigger_words = ", ".join(self.get_trigger_words())
        system_prompt_length = len(self.get_system_prompt())
        report.append(f"- Trigger words: {trigger_words}")
        report.append(f"- System prompt: {system_prompt_length} caracteres")

        report.append(f"\n## ğŸ“‹ MetodologÃ­a")
        report.append(f"- EvaluaciÃ³n manual caso por caso")
        report.append(f"- Criterio: Mejor captura de personalidad del bot configurado")
        report.append(f"- {len(self.get_test_scenarios())} escenarios de prueba")
        report.append(f"- {len(self.models)} modelos evaluados")

        return "\n".join(report)


def main():
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("âŒ Error: OPENROUTER_API_KEY no configurada")
        return

    benchmark = GordoBenchmark(api_key)

    print("ğŸ¯ Benchmark del Bot - EvaluaciÃ³n Manual")
    print("Evaluando capacidad de replicar personalidad configurada del bot")
    print("ğŸ‘¤ EvaluaciÃ³n humana caso por caso\n")

    # Ejecutar evaluaciÃ³n manual
    manual_scores = benchmark.run_manual_benchmark()

    # Generar reporte
    report = benchmark.generate_manual_report(manual_scores)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"benchmark_manual_report_{timestamp}.md"
    with open(report_filename, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"\nâœ… Benchmark completado!")
    print(f"ğŸ“‹ Reporte generado en: {report_filename}")
    print("\n" + "=" * 50)
    print(report)


if __name__ == "__main__":
    main()
